{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a72f6616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Hyperparameters\n",
    "N_STEPS = 10\n",
    "NUM_EPOCHS = 80\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 256\n",
    "PREFIX_LEN = 4\n",
    "MODEL_DIR = \"weights/roberta-diffusion-single-with-prefix\"\n",
    "SAVE_DIR = \"weights/roberta\"\n",
    "\n",
    "# linearly spaced mask probabilities from 1/N_STEPS → 1.0\n",
    "mask_probs = [(i + 1) / N_STEPS for i in range(N_STEPS - 1, -1, -1)]\n",
    "mask_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Load dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"larryvrh/Chinese-Poems\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a044161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to only 唐代 and 宋代 poems\n",
    "dataset['train'] = dataset['train'].filter(\n",
    "    lambda ex: ex[\"dynasty\"] in [\"唐代\", \"宋代\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd383aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dynasty', 'author', 'title', 'content']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b3741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForMaskedLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "# 2) Load model and tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODEL_DIR, max_len=MAX_LEN)\n",
    "tokenizer.model_max_length = MAX_LEN\n",
    "model = RobertaForMaskedLM.from_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb302409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 138771/138771 [00:16<00:00, 8597.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"content\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee09db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7d3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: [0, 41907, 46, 10659, 41907, 17772, 13859, 36484, 18537, 9357, 47842, 10809, 49075, 3602, 41907, 13859, 15375, 42393, 15389, 15389, 43251, 4394, 14285, 42393, 7471, 4333, 37127, 17772, 15722, 42393, 862, 6248, 47876, 9470, 36484, 16948, 10809, 36714, 5782, 6248, 49117, 20024, 45682, 50118, 41907, 48894, 42393, 15389, 13859, 48635, 12410, 36484, 27, 15113, 47504, 11582, 48820, 23171, 48128, 12410, 43251, 4394, 14285, 42393, 14292, 12410, 46499, 3602, 48186, 10674, 48341, 4394, 47876, 3602, 36484, 3070, 4958, 49117, 14292, 45682, 2]\n",
      "Length: 81\n",
      "---\n",
      "Sample 1: [0, 49874, 4333, 36484, 19002, 3070, 47856, 27819, 36484, 6248, 10172, 36714, 7258, 3602, 36484, 2840, 9253, 36714, 4394, 3070, 43251, 4394, 14285, 41907, 15375, 11582, 41907, 11582, 2469, 47089, 23171, 36714, 4333, 8210, 36484, 3602, 11582, 47954, 6248, 47240, 7487, 45682, 50118, 48617, 15264, 36484, 6248, 10172, 36484, 13859, 8210, 48607, 25448, 37127, 15389, 9264, 49212, 4333, 41907, 49794, 43251, 4394, 14285, 46499, 18164, 36714, 4726, 17772, 46015, 48, 48854, 12410, 48732, 17772, 42393, 7258, 18164, 47973, 45682, 2]\n",
      "Length: 80\n",
      "---\n",
      "Sample 2: [0, 47842, 711, 42393, 4333, 862, 48991, 10172, 36714, 4333, 2469, 44636, 10278, 37127, 10674, 19002, 36484, 3070, 4958, 43251, 4394, 14285, 37127, 10965, 9357, 48420, 9253, 42393, 15389, 15389, 37127, 17772, 6800, 48558, 3070, 36484, 6382, 10278, 46499, 14292, 45682, 50118, 48128, 6800, 36714, 3070, 8906, 37127, 2023, 5543, 47516, 27969, 48617, 12410, 48414, 12736, 36484, 20024, 3070, 43251, 4394, 14285, 36714, 11582, 10674, 36484, 49638, 47876, 16948, 48745, 7258, 48959, 6248, 36484, 6800, 9357, 49122, 4333, 45682, 2]\n",
      "Length: 80\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= MAX_LEN:\n",
    "        total_length = (total_length // MAX_LEN) * MAX_LEN\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + MAX_LEN] for i in range(0, total_length, MAX_LEN)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
